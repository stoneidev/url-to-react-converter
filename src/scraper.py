"""
ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ
Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì  í˜ì´ì§€ ë Œë”ë§ ë° ìì‚° ë‹¤ìš´ë¡œë“œ
"""

import asyncio
import os
from pathlib import Path
from typing import Dict, List, Set
from urllib.parse import urljoin, urlparse
import hashlib

from playwright.async_api import async_playwright, Page
import aiohttp
from bs4 import BeautifulSoup


class WebPageScraper:
    """ì›¹í˜ì´ì§€ ìŠ¤í¬ë˜í•‘ ë° ìì‚° ë‹¤ìš´ë¡œë“œ í´ë˜ìŠ¤"""

    def __init__(self, output_dir: str = "./output"):
        self.output_dir = Path(output_dir)
        self.assets_dir = self.output_dir / "assets"

        # ìì‚° ë””ë ‰í† ë¦¬ ìƒì„±
        self.css_dir = self.assets_dir / "css"
        self.js_dir = self.assets_dir / "js"
        self.images_dir = self.assets_dir / "images"

        for directory in [self.css_dir, self.js_dir, self.images_dir]:
            directory.mkdir(parents=True, exist_ok=True)

        # ìˆ˜ì§‘ëœ ìì‚° URL ì¶”ì 
        self.css_files: Set[str] = set()
        self.js_files: Set[str] = set()
        self.images: Set[str] = set()

        # ë‹¤ìš´ë¡œë“œ ë§¤í•‘ (ì›ë³¸ URL -> ë¡œì»¬ ê²½ë¡œ)
        self.url_to_local: Dict[str, str] = {}

    async def fetch_page(self, url: str) -> Dict:
        """
        URLì—ì„œ ë Œë”ë§ëœ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°

        Args:
            url: ìŠ¤í¬ë˜í•‘í•  URL

        Returns:
            Dict with 'html', 'css_files', 'js_files', 'images'
        """
        print(f"ğŸŒ Fetching page: {url}")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ëª¨ë‹ˆí„°ë§
            page.on('response', lambda response: self._track_assets(response))

            try:
                # í˜ì´ì§€ ë¡œë“œ
                await page.goto(url, wait_until='networkidle', timeout=30000)

                # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸ )
                await page.wait_for_timeout(2000)

                # ë Œë”ë§ëœ HTML ê°€ì ¸ì˜¤ê¸°
                html = await page.content()

                print(f"âœ… Page loaded successfully")
                print(f"   - CSS files: {len(self.css_files)}")
                print(f"   - JS files: {len(self.js_files)}")
                print(f"   - Images: {len(self.images)}")

                return {
                    'html': html,
                    'css_files': list(self.css_files),
                    'js_files': list(self.js_files),
                    'images': list(self.images),
                    'base_url': url
                }

            finally:
                await browser.close()

    def _track_assets(self, response):
        """ë„¤íŠ¸ì›Œí¬ ì‘ë‹µì—ì„œ ìì‚° íŒŒì¼ ì¶”ì """
        url = response.url
        content_type = response.headers.get('content-type', '').lower()

        # CSS íŒŒì¼
        if 'text/css' in content_type or url.endswith('.css'):
            self.css_files.add(url)

        # JavaScript íŒŒì¼
        elif 'javascript' in content_type or url.endswith('.js'):
            self.js_files.add(url)

        # ì´ë¯¸ì§€ íŒŒì¼
        elif 'image/' in content_type or any(url.endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp', '.ico']):
            self.images.add(url)

    async def download_assets(self, base_url: str) -> Dict[str, str]:
        """
        ëª¨ë“  ìì‚° íŒŒì¼ ë‹¤ìš´ë¡œë“œ

        Args:
            base_url: ê¸°ë³¸ URL (ìƒëŒ€ ê²½ë¡œ í•´ê²°ìš©)

        Returns:
            Dict mapping original URL to local file path
        """
        print(f"\nğŸ“¦ Downloading assets...")

        async with aiohttp.ClientSession() as session:
            tasks = []

            # CSS íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            for url in self.css_files:
                absolute_url = urljoin(base_url, url)
                tasks.append(self._download_file(session, absolute_url, self.css_dir, '.css'))

            # JS íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            for url in self.js_files:
                absolute_url = urljoin(base_url, url)
                tasks.append(self._download_file(session, absolute_url, self.js_dir, '.js'))

            # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
            for url in self.images:
                absolute_url = urljoin(base_url, url)
                tasks.append(self._download_file(session, absolute_url, self.images_dir))

            # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ
            await asyncio.gather(*tasks, return_exceptions=True)

        print(f"âœ… Assets downloaded: {len(self.url_to_local)} files")
        return self.url_to_local

    async def _download_file(
        self,
        session: aiohttp.ClientSession,
        url: str,
        output_dir: Path,
        force_extension: str = None
    ):
        """ê°œë³„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    content = await response.read()

                    # íŒŒì¼ëª… ìƒì„± (URL í•´ì‹œ ì‚¬ìš©)
                    url_hash = hashlib.md5(url.encode()).hexdigest()[:8]

                    # í™•ì¥ì ê²°ì •
                    if force_extension:
                        extension = force_extension
                    else:
                        parsed = urlparse(url)
                        extension = Path(parsed.path).suffix or '.bin'

                    filename = f"{url_hash}{extension}"
                    filepath = output_dir / filename

                    # íŒŒì¼ ì €ì¥
                    filepath.write_bytes(content)

                    # ë§¤í•‘ ì €ì¥ (ìƒëŒ€ ê²½ë¡œ)
                    relative_path = filepath.relative_to(self.output_dir)
                    self.url_to_local[url] = str(relative_path)

                    print(f"   âœ“ Downloaded: {filename}")

        except Exception as e:
            print(f"   âœ— Failed to download {url}: {e}")

    def _remove_hidden_elements(self, soup: BeautifulSoup) -> int:
        """
        display: noneì¸ ìºëŸ¬ì…€/ìŠ¬ë¼ì´ë” ìš”ì†Œ ì œê±°

        Args:
            soup: BeautifulSoup ê°ì²´

        Returns:
            ì œê±°ëœ ìš”ì†Œ ê°œìˆ˜
        """
        removed = 0

        # display: noneì„ ê°€ì§„ ëª¨ë“  ìš”ì†Œ ì°¾ê¸°
        # find_all()ì˜ ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ìˆ˜ì • ì¤‘ ìˆœíšŒ ë¬¸ì œ ë°©ì§€
        elements_with_style = list(soup.find_all(style=True))

        for element in elements_with_style:
            # elementê°€ ì´ë¯¸ ì œê±°ëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
            if not element or not hasattr(element, 'attrs') or not element.attrs:
                continue

            style = element.attrs.get('style', '')
            if not style:
                continue

            # display: none ì²´í¬ (ê³µë°± ë¬´ì‹œ)
            if 'display' in style and 'none' in style:
                # ê°„ë‹¨í•œ íŒŒì‹± (CSS íŒŒì„œ ì—†ì´)
                style_lower = style.lower().replace(' ', '')
                if 'display:none' in style_lower:
                    # ìš”ì†Œ ì œê±°
                    element.decompose()
                    removed += 1

        return removed

    def _remove_duplicate_scripts_and_links(self, soup: BeautifulSoup) -> int:
        """
        ì¤‘ë³µëœ script ë° link íƒœê·¸ ì œê±°

        Args:
            soup: BeautifulSoup ê°ì²´

        Returns:
            ì œê±°ëœ ìš”ì†Œ ê°œìˆ˜
        """
        removed = 0

        # ì¤‘ë³µ script íƒœê·¸ ì œê±°
        seen_scripts = set()
        for script in list(soup.find_all('script', src=True)):
            if not script or not hasattr(script, 'attrs'):
                continue

            src = script.attrs.get('src', '')
            if not src:
                continue

            if src in seen_scripts:
                # ì¤‘ë³µëœ ìŠ¤í¬ë¦½íŠ¸ ì œê±°
                script.decompose()
                removed += 1
            else:
                seen_scripts.add(src)

        # ì¤‘ë³µ link íƒœê·¸ ì œê±° (CSS)
        seen_links = set()
        for link in list(soup.find_all('link', href=True)):
            if not link or not hasattr(link, 'attrs'):
                continue

            href = link.attrs.get('href', '')
            rel = link.attrs.get('rel', [])
            if not href:
                continue

            # stylesheetë§Œ ì²´í¬
            if 'stylesheet' in rel or (isinstance(rel, list) and 'stylesheet' in rel):
                if href in seen_links:
                    # ì¤‘ë³µëœ ë§í¬ ì œê±°
                    link.decompose()
                    removed += 1
                else:
                    seen_links.add(href)

        return removed

    def replace_urls_in_html(self, html: str, base_url: str) -> str:
        """
        HTML ë‚´ì˜ URLì„ ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½

        Args:
            html: ì›ë³¸ HTML
            base_url: ê¸°ë³¸ URL

        Returns:
            ë¡œì»¬ ê²½ë¡œë¡œ ë³€ê²½ëœ HTML
        """
        print(f"\nğŸ”§ Replacing URLs with local paths...")

        soup = BeautifulSoup(html, 'html.parser')
        replacements = 0

        # CSS ë§í¬ ë³€ê²½
        for link in soup.find_all('link', rel='stylesheet'):
            if link.get('href'):
                original_url = urljoin(base_url, link['href'])
                if original_url in self.url_to_local:
                    link['href'] = self.url_to_local[original_url]
                    replacements += 1

        # JS ìŠ¤í¬ë¦½íŠ¸ ë³€ê²½
        for script in soup.find_all('script', src=True):
            original_url = urljoin(base_url, script['src'])
            if original_url in self.url_to_local:
                script['src'] = self.url_to_local[original_url]
                replacements += 1

        # ì´ë¯¸ì§€ ë³€ê²½
        for img in soup.find_all('img', src=True):
            original_url = urljoin(base_url, img['src'])
            if original_url in self.url_to_local:
                img['src'] = self.url_to_local[original_url]
                replacements += 1

        # srcset ì†ì„±ë„ ì²˜ë¦¬
        for img in soup.find_all('img', srcset=True):
            # srcsetì€ ë³µì¡í•˜ë¯€ë¡œ ê°„ë‹¨í•œ ê²½ìš°ë§Œ ì²˜ë¦¬
            pass  # TODO: srcset ì²˜ë¦¬

        print(f"âœ… Replaced {replacements} URLs")

        # ì¤‘ë³µ ìŠ¤í¬ë¦½íŠ¸/ë§í¬ ì œê±°
        print(f"\nğŸ”§ Removing duplicate scripts and links...")
        removed_duplicates = self._remove_duplicate_scripts_and_links(soup)
        print(f"âœ… Removed {removed_duplicates} duplicate script/link tags")

        # ìˆ¨ê²¨ì§„ ìºëŸ¬ì…€/ìŠ¬ë¼ì´ë” ìš”ì†Œ ì œê±°
        print(f"\nğŸ§¹ Removing hidden carousel elements...")
        removed_hidden = self._remove_hidden_elements(soup)
        print(f"âœ… Removed {removed_hidden} hidden elements")

        return str(soup)

    async def scrape_and_save(self, url: str, output_name: str = "index") -> Path:
        """
        ì™„ì „í•œ ìŠ¤í¬ë˜í•‘ ë° ì €ì¥ í”„ë¡œì„¸ìŠ¤

        Args:
            url: ìŠ¤í¬ë˜í•‘í•  URL
            output_name: ì¶œë ¥ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)

        Returns:
            ì €ì¥ëœ HTML íŒŒì¼ ê²½ë¡œ
        """
        # 1. í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
        page_data = await self.fetch_page(url)

        # 2. ìì‚° ë‹¤ìš´ë¡œë“œ
        await self.download_assets(page_data['base_url'])

        # 3. HTML ë‚´ URL ë³€ê²½
        modified_html = self.replace_urls_in_html(
            page_data['html'],
            page_data['base_url']
        )

        # 4. HTML íŒŒì¼ ì €ì¥
        html_path = self.output_dir / f"{output_name}.html"
        html_path.write_text(modified_html, encoding='utf-8')

        print(f"\nâœ¨ Scraping complete!")
        print(f"   ğŸ“„ HTML saved: {html_path}")
        print(f"   ğŸ“ Assets dir: {self.assets_dir}")

        return html_path


# CLI ì‚¬ìš©ì„ ìœ„í•œ ë©”ì¸ í•¨ìˆ˜
async def main():
    """CLIì—ì„œ ì§ì ‘ ì‹¤í–‰ì‹œ"""
    import sys

    if len(sys.argv) < 2:
        print("Usage: python scraper.py <url> [output_name]")
        sys.exit(1)

    url = sys.argv[1]
    output_name = sys.argv[2] if len(sys.argv) > 2 else "index"

    scraper = WebPageScraper()
    await scraper.scrape_and_save(url, output_name)


if __name__ == "__main__":
    asyncio.run(main())
